{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "import collections\n",
    "\n",
    "class DecisionTree:\n",
    "\t\"\"\"Binary tree implementation with true and false branch. \"\"\"\n",
    "\tdef __init__(self, col=-1, value=None, trueBranch=None, falseBranch=None, results=None):\n",
    "\t\tself.col = col\n",
    "\t\tself.value = value\n",
    "\t\tself.trueBranch = trueBranch\n",
    "\t\tself.falseBranch = falseBranch\n",
    "\t\tself.results = results # None for nodes, not None for leaves\n",
    "\n",
    "\n",
    "def divideSet(rows, column, value):\n",
    "\tsplittingFunction = None\n",
    "\tif isinstance(value, int) or isinstance(value, float): # for int and float values\n",
    "\t\tsplittingFunction = lambda row : row[column] >= value\n",
    "\telse: # for strings \n",
    "\t\tsplittingFunction = lambda row : row[column] == value\n",
    "\tlist1 = [row for row in rows if splittingFunction(row)]\n",
    "\tlist2 = [row for row in rows if not splittingFunction(row)]\n",
    "\treturn (list1, list2)\n",
    "\n",
    "\n",
    "def uniqueCounts(rows):\n",
    "\tresults = {}\n",
    "\tfor row in rows:\n",
    "\t\tr = row[-1]\n",
    "\t\tif r not in results: results[r] = 0\n",
    "\t\tresults[r] += 1\n",
    "\treturn results\n",
    "\n",
    "\n",
    "def entropy(rows):\n",
    "\tfrom math import log\n",
    "\tlog2 = lambda x: log(x)/log(2)\n",
    "\tresults = uniqueCounts(rows)\n",
    "\n",
    "\tentr = 0.0\n",
    "\tfor r in results:\n",
    "\t\tp = float(results[r])/len(rows)\n",
    "\t\tentr -= p*log2(p)\n",
    "\treturn entr\n",
    "\n",
    "\n",
    "def gini(rows):\n",
    "\ttotal = len(rows)\n",
    "\tcounts = uniqueCounts(rows)\n",
    "\timp = 0.0\n",
    "\n",
    "\tfor k1 in counts:\n",
    "\t\tp1 = float(counts[k1])/total  \n",
    "\t\tfor k2 in counts:\n",
    "\t\t\tif k1 == k2: continue\n",
    "\t\t\tp2 = float(counts[k2])/total\n",
    "\t\t\timp += p1*p2\n",
    "\treturn imp\n",
    "\n",
    "\n",
    "def variance(rows):\n",
    "\tif len(rows) == 0: return 0\n",
    "\tdata = [float(row[len(row) - 1]) for row in rows]\n",
    "\tmean = sum(data) / len(data)\n",
    "\n",
    "\tvariance = sum([(d-mean)**2 for d in data]) / len(data)\n",
    "\treturn variance\n",
    "\n",
    "\n",
    "def growDecisionTreeFrom(rows, evaluationFunction=entropy):\n",
    "\t\"\"\"Grows and then returns a binary decision tree. \n",
    "\tevaluationFunction: entropy or gini\"\"\" \n",
    "\n",
    "\tif len(rows) == 0: return DecisionTree()\n",
    "\tcurrentScore = evaluationFunction(rows)\n",
    "\n",
    "\tbestGain = 0.0\n",
    "\tbestAttribute = None\n",
    "\tbestSets = None\n",
    "\n",
    "\tcolumnCount = len(rows[0]) - 1  # last column is the result/target column\n",
    "\tfor col in range(0, columnCount):\n",
    "\t\tcolumnValues = [row[col] for row in rows]\n",
    "\n",
    "\t\tfor value in columnValues:\n",
    "\t\t\t(set1, set2) = divideSet(rows, col, value)\n",
    "\n",
    "\t\t\t# Gain -- Entropy or Gini\n",
    "\t\t\tp = float(len(set1)) / len(rows)\n",
    "\t\t\tgain = currentScore - p*evaluationFunction(set1) - (1-p)*evaluationFunction(set2)\n",
    "\t\t\tif gain>bestGain and len(set1)>0 and len(set2)>0:\n",
    "\t\t\t\tbestGain = gain\n",
    "\t\t\t\tbestAttribute = (col, value)\n",
    "\t\t\t\tbestSets = (set1, set2)\n",
    "\n",
    "\tif bestGain > 0:\n",
    "\t\ttrueBranch = growDecisionTreeFrom(bestSets[0])\n",
    "\t\tfalseBranch = growDecisionTreeFrom(bestSets[1])\n",
    "\t\treturn DecisionTree(col=bestAttribute[0], value=bestAttribute[1], trueBranch=trueBranch, falseBranch=falseBranch)\n",
    "\telse:\n",
    "\t\treturn DecisionTree(results=uniqueCounts(rows))\n",
    "\n",
    "\n",
    "def prune(tree, minGain, evaluationFunction=entropy, notify=False):\n",
    "\t\"\"\"Prunes the obtained tree according to the minimal gain (entropy or Gini). \"\"\"\n",
    "\t# recursive call for each branch\n",
    "\tif tree.trueBranch.results == None: prune(tree.trueBranch, minGain, evaluationFunction, notify)\n",
    "\tif tree.falseBranch.results == None: prune(tree.falseBranch, minGain, evaluationFunction, notify)\n",
    "\n",
    "\t# merge leaves (potentionally)\n",
    "\tif tree.trueBranch.results != None and tree.falseBranch.results != None:\n",
    "\t\ttb, fb = [], []\n",
    "\n",
    "\t\tfor v, c in tree.trueBranch.results.items(): tb += [[v]] * c\n",
    "\t\tfor v, c in tree.falseBranch.results.items(): fb += [[v]] * c\n",
    "\n",
    "\t\tp = float(len(tb)) / len(tb + fb)\n",
    "\t\tdelta = evaluationFunction(tb+fb) - p*evaluationFunction(tb) - (1-p)*evaluationFunction(fb)\n",
    "\t\tif delta < minGain:\t\n",
    "\t\t\tif notify: print('A branch was pruned: gain = %f' % delta)\t\t\n",
    "\t\t\ttree.trueBranch, tree.falseBranch = None, None\n",
    "\t\t\ttree.results = uniqueCounts(tb + fb)\n",
    "\n",
    "\n",
    "def classify(observations, tree, dataMissing=False):\n",
    "\t\"\"\"Classifies the observationss according to the tree.\n",
    "\tdataMissing: true or false if data are missing or not. \"\"\"\n",
    "\n",
    "\tdef classifyWithoutMissingData(observations, tree):\n",
    "\t\tif tree.results != None:  # leaf\n",
    "\t\t\treturn tree.results\n",
    "\t\telse:\n",
    "\t\t\tv = observations[tree.col]\n",
    "\t\t\tbranch = None\n",
    "\t\t\tif isinstance(v, int) or isinstance(v, float):\n",
    "\t\t\t\tif v >= tree.value: branch = tree.trueBranch\n",
    "\t\t\t\telse: branch = tree.falseBranch\n",
    "\t\t\telse:\n",
    "\t\t\t\tif v == tree.value: branch = tree.trueBranch\n",
    "\t\t\t\telse: branch = tree.falseBranch\n",
    "\t\treturn classifyWithoutMissingData(observations, branch)\n",
    "\n",
    "\n",
    "\tdef classifyWithMissingData(observations, tree):\n",
    "\t\tif tree.results != None:  # leaf \n",
    "\t\t\treturn tree.results\n",
    "\t\telse:\n",
    "\t\t\tv = observations[tree.col]\n",
    "\t\t\tif v == None:\n",
    "\t\t\t\ttr = classifyWithMissingData(observations, tree.trueBranch)\n",
    "\t\t\t\tfr = classifyWithMissingData(observations, tree.falseBranch)\n",
    "\t\t\t\ttcount = sum(tr.values())\n",
    "\t\t\t\tfcount = sum(fr.values())\n",
    "\t\t\t\ttw = float(tcount)/(tcount + fcount)\n",
    "\t\t\t\tfw = float(fcount)/(tcount + fcount)\n",
    "\t\t\t\tresult = collections.defaultdict(int) # Problem description: http://blog.ludovf.net/python-collections-defaultdict/\n",
    "\t\t\t\tfor k, v in tr.items(): result[k] += v*tw\n",
    "\t\t\t\tfor k, v in fr.items(): result[k] += v*fw\n",
    "\t\t\t\treturn dict(result)\n",
    "\t\t\telse:\n",
    "\t\t\t\tbranch = None\n",
    "\t\t\t\tif isinstance(v, int) or isinstance(v, float):\n",
    "\t\t\t\t\tif v >= tree.value: branch = tree.trueBranch\n",
    "\t\t\t\t\telse: branch = tree.falseBranch\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tif v == tree.value: branch = tree.trueBranch\n",
    "\t\t\t\t\telse: branch = tree.falseBranch\n",
    "\t\t\treturn classifyWithMissingData(observations, branch)\n",
    "\n",
    "\t# function body\n",
    "\tif dataMissing: \n",
    "\t\treturn classifyWithMissingData(observations, tree)\n",
    "\telse: \n",
    "\t\treturn classifyWithoutMissingData(observations, tree)\n",
    "\n",
    "\n",
    "def plot(decisionTree):\n",
    "\t\"\"\"Plots the obtained decision tree. \"\"\"\n",
    "\tdef toString(decisionTree, indent=''):\n",
    "\t\tif decisionTree.results != None:  # leaf node\n",
    "\t\t\treturn str(decisionTree.results)\n",
    "\t\telse:\n",
    "\t\t\tif isinstance(decisionTree.value, int) or isinstance(decisionTree.value, float):\n",
    "\t\t\t\tdecision = 'Column %s: x >= %s?' % (decisionTree.col, decisionTree.value)\n",
    "\t\t\telse:\n",
    "\t\t\t\tdecision = 'Column %s: x == %s?' % (decisionTree.col, decisionTree.value)\n",
    "\t\t\ttrueBranch = indent + 'yes -> ' + toString(decisionTree.trueBranch, indent + '\\t\\t')\n",
    "\t\t\tfalseBranch = indent + 'no  -> ' + toString(decisionTree.falseBranch, indent + '\\t\\t')\n",
    "\t\t\treturn (decision + '\\n' + trueBranch + '\\n' + falseBranch)\n",
    "\n",
    "\tprint(toString(decisionTree))\n",
    "\n",
    "\n",
    "def loadCSV(file):\n",
    "\t\"\"\"Loads a CSV file and converts all floats and ints into basic datatypes.\"\"\" \n",
    "\tdef convertTypes(s):\n",
    "\t\ts = s.strip()\n",
    "\t\ttry:\n",
    "\t\t\treturn float(s) if '.' in s else int(s)\n",
    "\t\texcept ValueError:\n",
    "\t\t\treturn s\t\n",
    "\n",
    "\treader = csv.reader(open(file, 'rt'))\n",
    "\treturn [[convertTypes(item) for item in row] for row in reader]\n",
    "\t\t\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\t# Select the example you want to classify\n",
    "\texample = 1\n",
    "\n",
    "\t# All examples do the following steps:\n",
    "\t# \t1. Load training data\n",
    "\t# \t2. Let the decision tree grow\n",
    "\t# \t4. Plot the decision tree\n",
    "\t# \t5. classify without missing data\n",
    "\t# \t6. Classifiy with missing data\n",
    "\t# \t(7.) Prune the decision tree according to a minimal gain level\n",
    "\t# \t(8.) Plot the pruned tree\n",
    "\n",
    "\tif example == 1:\n",
    "\t\t# the smaller examples\n",
    "\t\ttrainingData = loadCSV('tbc.csv') # sorry for not translating the TBC and pneumonia symptoms\n",
    "\t\tdecisionTree = growDecisionTreeFrom(trainingData)\n",
    "\t\t#decisionTree = growDecisionTreeFrom(trainingData, evaluationFunction=gini) # with gini\n",
    "\t\tplot(decisionTree) \n",
    "\n",
    "\t\tprint(classify(['ohne', 'leicht', 'Streifen', 'normal', 'normal'], decisionTree, dataMissing=False)) \n",
    "\t\tprint(classify([None, 'leicht', None, 'Flocken', 'fiepend'], decisionTree, dataMissing=True)) # no longer unique\n",
    "\n",
    "\t\t# Don' forget if you compare the resulting tree with the tree in my presentation: here it is a binary tree!\n",
    "\n",
    "\telse:\n",
    "\t\t# the bigger example\n",
    "\t\ttrainingData = loadCSV('fishiris.csv') # demo data from matlab\n",
    "\t\tdecisionTree = growDecisionTreeFrom(trainingData)\t\t\n",
    "\t\tplot(decisionTree)\n",
    "\n",
    "\t\tprune(decisionTree, 0.5, notify=True) # notify, when a branch is pruned (one time in this example)\n",
    "\t\tplot(decisionTree)\n",
    "\n",
    "\t\tprint(classify([6.0, 2.2, 5.0, 1.5], decisionTree)) # dataMissing=False is the default setting\n",
    "\t\tprint(classify([None, None, None, 1.5], decisionTree, dataMissing=True)) # no longer unique\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'Python Interactive'",
   "language": "python",
   "name": "d72ef99e-de46-4c8b-a1fc-646e93c6dd6d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
