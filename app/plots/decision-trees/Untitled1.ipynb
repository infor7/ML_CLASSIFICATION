{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 2: x >= 3?\n",
      "yes -> Column 3: x >= 1.8?\n",
      "\t\tyes -> Column 2: x >= 4.9?\n",
      "\t\t\t\tyes -> {'virginica': 43}\n",
      "\t\t\t\tno  -> Column 0: x >= 6?\n",
      "\t\t\t\t\t\tyes -> {'virginica': 2}\n",
      "\t\t\t\t\t\tno  -> {'versicolor': 1}\n",
      "\t\tno  -> Column 2: x >= 5?\n",
      "\t\t\t\tyes -> Column 3: x >= 1.6?\n",
      "\t\t\t\t\t\tyes -> Column 0: x >= 7.2?\n",
      "\t\t\t\t\t\t\t\tyes -> {'virginica': 1}\n",
      "\t\t\t\t\t\t\t\tno  -> {'versicolor': 2}\n",
      "\t\t\t\t\t\tno  -> {'virginica': 3}\n",
      "\t\t\t\tno  -> Column 3: x >= 1.7?\n",
      "\t\t\t\t\t\tyes -> {'virginica': 1}\n",
      "\t\t\t\t\t\tno  -> {'versicolor': 47}\n",
      "no  -> {'setosa': 50}\n",
      "A branch was pruned: gain = 0.146094\n",
      "Column 2: x >= 3?\n",
      "yes -> Column 3: x >= 1.8?\n",
      "\t\tyes -> Column 2: x >= 4.9?\n",
      "\t\t\t\tyes -> {'virginica': 43}\n",
      "\t\t\t\tno  -> Column 0: x >= 6?\n",
      "\t\t\t\t\t\tyes -> {'virginica': 2}\n",
      "\t\t\t\t\t\tno  -> {'versicolor': 1}\n",
      "\t\tno  -> Column 2: x >= 5?\n",
      "\t\t\t\tyes -> Column 3: x >= 1.6?\n",
      "\t\t\t\t\t\tyes -> Column 0: x >= 7.2?\n",
      "\t\t\t\t\t\t\t\tyes -> {'virginica': 1}\n",
      "\t\t\t\t\t\t\t\tno  -> {'versicolor': 2}\n",
      "\t\t\t\t\t\tno  -> {'virginica': 3}\n",
      "\t\t\t\tno  -> {'virginica': 1, 'versicolor': 47}\n",
      "no  -> {'setosa': 50}\n",
      "{'virginica': 3}\n",
      "{'virginica': 0.5315890699277861, 'versicolor': 21.039735820299743, 'setosa': 26.218383713756943}\n"
     ]
    }
   ],
   "source": [
    "import csv \n",
    "import collections\n",
    "\n",
    "class DecisionTree:\n",
    "    \"\"\"Binary tree implementation with true and false branch. \"\"\"\n",
    "    def __init__(self, col=-1, value=None, trueBranch=None, falseBranch=None, results=None):\n",
    "        self.col = col\n",
    "        self.value = value\n",
    "        self.trueBranch = trueBranch\n",
    "        self.falseBranch = falseBranch\n",
    "        self.results = results # None for nodes, not None for leaves\n",
    "\n",
    "\n",
    "def divideSet(rows, column, value):\n",
    "    splittingFunction = None\n",
    "    if isinstance(value, int) or isinstance(value, float): # for int and float values\n",
    "        splittingFunction = lambda row : row[column] >= value\n",
    "    else: # for strings \n",
    "        splittingFunction = lambda row : row[column] == value\n",
    "    list1 = [row for row in rows if splittingFunction(row)]\n",
    "    list2 = [row for row in rows if not splittingFunction(row)]\n",
    "    return (list1, list2)\n",
    "\n",
    "\n",
    "def uniqueCounts(rows):\n",
    "    results = {}\n",
    "    for row in rows:\n",
    "        r = row[-1]\n",
    "        if r not in results: results[r] = 0\n",
    "        results[r] += 1\n",
    "    return results\n",
    "\n",
    "\n",
    "def entropy(rows):\n",
    "    from math import log\n",
    "    log2 = lambda x: log(x)/log(2)\n",
    "    results = uniqueCounts(rows)\n",
    "\n",
    "    entr = 0.0\n",
    "    for r in results:\n",
    "        p = float(results[r])/len(rows)\n",
    "        entr -= p*log2(p)\n",
    "    return entr\n",
    "\n",
    "\n",
    "def gini(rows):\n",
    "    total = len(rows)\n",
    "    counts = uniqueCounts(rows)\n",
    "    imp = 0.0\n",
    "\n",
    "    for k1 in counts:\n",
    "        p1 = float(counts[k1])/total  \n",
    "        for k2 in counts:\n",
    "            if k1 == k2: continue\n",
    "            p2 = float(counts[k2])/total\n",
    "            imp += p1*p2\n",
    "    return imp\n",
    "\n",
    "\n",
    "def variance(rows):\n",
    "    if len(rows) == 0: return 0\n",
    "    data = [float(row[len(row) - 1]) for row in rows]\n",
    "    mean = sum(data) / len(data)\n",
    "\n",
    "    variance = sum([(d-mean)**2 for d in data]) / len(data)\n",
    "    return variance\n",
    "\n",
    "\n",
    "def growDecisionTreeFrom(rows, evaluationFunction=entropy):\n",
    "    \"\"\"Grows and then returns a binary decision tree. \n",
    "    evaluationFunction: entropy or gini\"\"\" \n",
    "\n",
    "    if len(rows) == 0: return DecisionTree()\n",
    "    currentScore = evaluationFunction(rows)\n",
    "\n",
    "    bestGain = 0.0\n",
    "    bestAttribute = None\n",
    "    bestSets = None\n",
    "\n",
    "    columnCount = len(rows[0]) - 1  # last column is the result/target column\n",
    "    for col in range(0, columnCount):\n",
    "        columnValues = [row[col] for row in rows]\n",
    "\n",
    "        for value in columnValues:\n",
    "            (set1, set2) = divideSet(rows, col, value)\n",
    "\n",
    "            # Gain -- Entropy or Gini\n",
    "            p = float(len(set1)) / len(rows)\n",
    "            gain = currentScore - p*evaluationFunction(set1) - (1-p)*evaluationFunction(set2)\n",
    "            if gain>bestGain and len(set1)>0 and len(set2)>0:\n",
    "                bestGain = gain\n",
    "                bestAttribute = (col, value)\n",
    "                bestSets = (set1, set2)\n",
    "\n",
    "    if bestGain > 0:\n",
    "        trueBranch = growDecisionTreeFrom(bestSets[0])\n",
    "        falseBranch = growDecisionTreeFrom(bestSets[1])\n",
    "        return DecisionTree(col=bestAttribute[0], value=bestAttribute[1], trueBranch=trueBranch, falseBranch=falseBranch)\n",
    "    else:\n",
    "        return DecisionTree(results=uniqueCounts(rows))\n",
    "\n",
    "\n",
    "def prune(tree, minGain, evaluationFunction=entropy, notify=False):\n",
    "    \"\"\"Prunes the obtained tree according to the minimal gain (entropy or Gini). \"\"\"\n",
    "    # recursive call for each branch\n",
    "    if tree.trueBranch.results == None: prune(tree.trueBranch, minGain, evaluationFunction, notify)\n",
    "    if tree.falseBranch.results == None: prune(tree.falseBranch, minGain, evaluationFunction, notify)\n",
    "\n",
    "    # merge leaves (potentionally)\n",
    "    if tree.trueBranch.results != None and tree.falseBranch.results != None:\n",
    "        tb, fb = [], []\n",
    "\n",
    "        for v, c in tree.trueBranch.results.items(): tb += [[v]] * c\n",
    "        for v, c in tree.falseBranch.results.items(): fb += [[v]] * c\n",
    "\n",
    "        p = float(len(tb)) / len(tb + fb)\n",
    "        delta = evaluationFunction(tb+fb) - p*evaluationFunction(tb) - (1-p)*evaluationFunction(fb)\n",
    "        if delta < minGain:    \n",
    "            if notify: print('A branch was pruned: gain = %f' % delta)        \n",
    "            tree.trueBranch, tree.falseBranch = None, None\n",
    "            tree.results = uniqueCounts(tb + fb)\n",
    "\n",
    "\n",
    "def classify(observations, tree, dataMissing=False):\n",
    "    \"\"\"Classifies the observationss according to the tree.\n",
    "    dataMissing: true or false if data are missing or not. \"\"\"\n",
    "\n",
    "    def classifyWithoutMissingData(observations, tree):\n",
    "        if tree.results != None:  # leaf\n",
    "            return tree.results\n",
    "        else:\n",
    "            v = observations[tree.col]\n",
    "            branch = None\n",
    "            if isinstance(v, int) or isinstance(v, float):\n",
    "                if v >= tree.value: branch = tree.trueBranch\n",
    "                else: branch = tree.falseBranch\n",
    "            else:\n",
    "                if v == tree.value: branch = tree.trueBranch\n",
    "                else: branch = tree.falseBranch\n",
    "        return classifyWithoutMissingData(observations, branch)\n",
    "\n",
    "\n",
    "    def classifyWithMissingData(observations, tree):\n",
    "        if tree.results != None:  # leaf \n",
    "            return tree.results\n",
    "        else:\n",
    "            v = observations[tree.col]\n",
    "            if v == None:\n",
    "                tr = classifyWithMissingData(observations, tree.trueBranch)\n",
    "                fr = classifyWithMissingData(observations, tree.falseBranch)\n",
    "                tcount = sum(tr.values())\n",
    "                fcount = sum(fr.values())\n",
    "                tw = float(tcount)/(tcount + fcount)\n",
    "                fw = float(fcount)/(tcount + fcount)\n",
    "                result = collections.defaultdict(int) # Problem description: http://blog.ludovf.net/python-collections-defaultdict/\n",
    "                for k, v in tr.items(): result[k] += v*tw\n",
    "                for k, v in fr.items(): result[k] += v*fw\n",
    "                return dict(result)\n",
    "            else:\n",
    "                branch = None\n",
    "                if isinstance(v, int) or isinstance(v, float):\n",
    "                    if v >= tree.value: branch = tree.trueBranch\n",
    "                    else: branch = tree.falseBranch\n",
    "                else:\n",
    "                    if v == tree.value: branch = tree.trueBranch\n",
    "                    else: branch = tree.falseBranch\n",
    "            return classifyWithMissingData(observations, branch)\n",
    "\n",
    "    # function body\n",
    "    if dataMissing: \n",
    "        return classifyWithMissingData(observations, tree)\n",
    "    else: \n",
    "        return classifyWithoutMissingData(observations, tree)\n",
    "\n",
    "\n",
    "def plot(decisionTree):\n",
    "    \"\"\"Plots the obtained decision tree. \"\"\"\n",
    "    def toString(decisionTree, indent=''):\n",
    "        if decisionTree.results != None:  # leaf node\n",
    "            return str(decisionTree.results)\n",
    "        else:\n",
    "            if isinstance(decisionTree.value, int) or isinstance(decisionTree.value, float):\n",
    "                decision = 'Column %s: x >= %s?' % (decisionTree.col, decisionTree.value)\n",
    "            else:\n",
    "                decision = 'Column %s: x == %s?' % (decisionTree.col, decisionTree.value)\n",
    "            trueBranch = indent + 'yes -> ' + toString(decisionTree.trueBranch, indent + '\\t\\t')\n",
    "            falseBranch = indent + 'no  -> ' + toString(decisionTree.falseBranch, indent + '\\t\\t')\n",
    "            return (decision + '\\n' + trueBranch + '\\n' + falseBranch)\n",
    "\n",
    "    print(toString(decisionTree))\n",
    "\n",
    "\n",
    "def loadCSV(file):\n",
    "    \"\"\"Loads a CSV file and converts all floats and ints into basic datatypes.\"\"\" \n",
    "    def convertTypes(s):\n",
    "        s = s.strip()\n",
    "        try:\n",
    "            return float(s) if '.' in s else int(s)\n",
    "        except ValueError:\n",
    "            return s    \n",
    "\n",
    "    reader = csv.reader(open(file, 'rt'))\n",
    "    return [[convertTypes(item) for item in row] for row in reader]\n",
    "        \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Select the example you want to classify\n",
    "    example = 2\n",
    "\n",
    "    # All examples do the following steps:\n",
    "    #     1. Load training data\n",
    "    #     2. Let the decision tree grow\n",
    "    #     4. Plot the decision tree\n",
    "    #     5. classify without missing data\n",
    "#     6. Classifiy with missing data\n",
    "    #     (7.) Prune the decision tree according to a minimal gain level\n",
    "    #     (8.) Plot the pruned tree\n",
    "\n",
    "    if example == 1:\n",
    "        # the smaller examples\n",
    "        trainingData = loadCSV('tbc.csv') # sorry for not translating the TBC and pneumonia symptoms\n",
    "        decisionTree = growDecisionTreeFrom(trainingData)\n",
    "        #decisionTree = growDecisionTreeFrom(trainingData, evaluationFunction=gini) # with gini\n",
    "        plot(decisionTree) \n",
    "\n",
    "        print(classify(['ohne', 'leicht', 'Streifen', 'normal', 'normal'], decisionTree, dataMissing=False)) \n",
    "        print(classify([None, 'leicht', None, 'Flocken', 'fiepend'], decisionTree, dataMissing=True)) # no longer unique\n",
    "\n",
    "        # Don' forget if you compare the resulting tree with the tree in my presentation: here it is a binary tree!\n",
    "\n",
    "    else:\n",
    "        # the bigger example\n",
    "        trainingData = loadCSV('fishiris.csv') # demo data from matlab\n",
    "        decisionTree = growDecisionTreeFrom(trainingData)        \n",
    "        plot(decisionTree)\n",
    "\n",
    "        prune(decisionTree, 0.5, notify=True) # notify, when a branch is pruned (one time in this example)\n",
    "        plot(decisionTree)\n",
    "\n",
    "        print(classify([6.0, 2.2, 5.0, 1.5], decisionTree)) # dataMissing=False is the default setting\n",
    "        print(classify([None, None, None, 1.5], decisionTree, dataMissing=True)) # no longer unique\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
